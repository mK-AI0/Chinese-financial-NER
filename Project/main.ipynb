{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据集预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、将训练集的mentions转换为text对应的标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取训练集\n",
    "df = pd.read_excel(\"data/train.xlsx\")\n",
    "\n",
    "# entity的起始和后续标记\n",
    "B_ENT = 1\n",
    "I_ENT = 2\n",
    "\n",
    "# 逐个样本处理\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # mentions的列表\n",
    "    mention_list = eval(row['mentions'])\n",
    "\n",
    "    # text字符串的长度\n",
    "    text_len = len(row['text'])\n",
    "\n",
    "    # 创建与text等长的label\n",
    "    label = [0]*text_len\n",
    "\n",
    "    # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "    for mention in mention_list:\n",
    "\n",
    "        # mention字典中的'offset'字段表示某个实体的开始和结束位置，左闭右开\n",
    "        (start, end) = eval(mention['offset'])\n",
    "\n",
    "        # entity的起始位置标记为B_ENT\n",
    "        label[start] = B_ENT\n",
    "        \n",
    "        # entity的其余位置标记为I_ENT\n",
    "        for num in range(start+1, end):\n",
    "            label[num] = I_ENT\n",
    "\n",
    "    # 存储label\n",
    "    df.loc[index, \"labels\"] = str(label)\n",
    "    \n",
    "# 保存为新的CSV文件\n",
    "df.to_csv(\"data/train_labeled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、分句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案一：用句号划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('data/train_labeled.csv')\n",
    "\n",
    "# 以句号为分隔\n",
    "char_sep = '。'\n",
    "li = []\n",
    "\n",
    "for index, row in df0.iterrows():\n",
    "    # indices是分隔的位置list，第一句从头开始\n",
    "    indices = [-1]\n",
    "\n",
    "    text = row['text']\n",
    "    text_len = len(text)\n",
    "\n",
    "    # 找到所有的句号\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == char_sep:\n",
    "            indices.append(i)\n",
    "\n",
    "    # 如果结尾不是句号，添加最后一个字符为分隔\n",
    "    if indices[-1] != text_len - 1:\n",
    "        indices.append(text_len - 1)\n",
    "\n",
    "    # 逐句处理\n",
    "    for j in range(len(indices)-1):\n",
    "        begin = indices[j] + 1\n",
    "        end = indices[j+1] + 1\n",
    "        sent = text[begin: end]\n",
    "        label = eval(row['labels'])[begin: end]\n",
    "        length = end - begin\n",
    "        li.append([length, sent, label])\n",
    "    \n",
    "df = pd.DataFrame(li, columns = ['length', 'text', 'labels'])\n",
    "\n",
    "df.to_csv('data/sep_by_。.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案二、指定最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_labeled.csv')\n",
    "df_s = pd.DataFrame(columns = ['segment_number','id','segment_len','text','mentions','labels'])\n",
    "max_size = 100\n",
    "#句段长度上限\n",
    "#print(df_s.shape)\n",
    "#header = 0 把第一行作为列名称，首位空缺设置为默认的0\n",
    "#print(df.loc[0])\n",
    "#显示列标\n",
    "#预期效果：每行是一个短于max_size样本片段，一个样本占有若干行（若干片段）。\n",
    "#预期效果：每列是一个片段的信息：\n",
    "#         列标：0,               1，        2，      3，      4，          5，          \n",
    "#              所属样本第几个片段 所属样本id 片段长度 片段文本  片段mentions  片段labels\n",
    "seg_num = 0\n",
    "#总片段下标\n",
    "\n",
    "for row in df.values:\n",
    "    #print(type(row),row.shape)\n",
    "    mention_list = eval(row[3])\n",
    "    #print(eval(row[5]))\n",
    "    #al_len = 0\n",
    "    al_seg_len = 0\n",
    "    segs = 0\n",
    "    #该文本已经切割的片段总长度和片段数目\n",
    "    \n",
    "    df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "    #df_s = df_s.append(pd.DataFrame([[segs,row[1],0,'',[],'']],columns = df_s.columns))\n",
    "    #print(type(df_s.loc[seg_num]))\n",
    "    #初始化该文本第一个片段\n",
    "    for i in range(len(mention_list)):\n",
    "    # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "        \n",
    "        start, end = eval(mention_list[i]['offset'])\n",
    "        # mention字典中的'offset'字段表示某个实体的开始和结束位置，左闭右开\n",
    "        if end < al_seg_len + max_size:\n",
    "        #实体全部在长度范围内\n",
    "            df_s.iloc[seg_num,4].append(mention_list[i])\n",
    "            #print(mention_list[i])\n",
    "            if i == len(mention_list):\n",
    "            #最后一个mention\n",
    "                df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,len(row[2]))]\n",
    "                df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,len(eval(row[5])))])\n",
    "                al_seg_len = min(al_seg_len + max_size,len(row[2]))\n",
    "\n",
    "                df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "                seg_num +=1   \n",
    "                segs += 1\n",
    "        else:\n",
    "        #过长，该实体放入下一个片段,开始整理已经确定的片段\n",
    "            i -= 1\n",
    "            #下一次还要放入该mention\n",
    "            df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,start)]\n",
    "            df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,start)])\n",
    "            \n",
    "            al_seg_len = min(al_seg_len + max_size,start)\n",
    "\n",
    "            df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "            seg_num +=1   \n",
    "            segs += 1\n",
    "            if al_seg_len < len(row[2]): df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "            #新片段\n",
    "    #最后处理尾部无mentions区域\n",
    "    while al_seg_len < len(row[2]):\n",
    "        df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,len(row[2]))]\n",
    "        df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,len(eval(row[5])))])\n",
    "        al_seg_len = min(al_seg_len + max_size,len(row[2]))\n",
    "        if len(eval(df_s.iloc[seg_num,5])) != len(df_s.iloc[seg_num,3]) or len(eval(df_s.iloc[seg_num,5]))>max_size: \n",
    "           print(len(df_s.iloc[seg_num,3]),len(eval(df_s.iloc[seg_num,5])))\n",
    "        df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "        seg_num +=1   \n",
    "        segs += 1\n",
    "        if al_seg_len < len(row[2]): df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "    #print(df_s.loc[0])\n",
    "\n",
    "df_s.to_csv(\"data/train_separated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载分词器并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 测试文本\n",
    "text = [\"红土创新基金管理有限公司6月30日发布公告，红土创新盐田港REIT(180301)的基金经理新聘陈超。\"\n",
    ",\"金能科技: 金能科技股份有限公司关于“金能转债”转股价格调整的提示性公告\"]\n",
    "\n",
    "# 将文本进行逐字分解\n",
    "def split_sent(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "def split(text_list):\n",
    "\n",
    "    text_split = []\n",
    "    for sent in text_list:\n",
    "        sent_split = split_sent(sent)\n",
    "        text_split.append(sent_split)\n",
    "    return text_split\n",
    "\n",
    "\n",
    "# 分解并编码\n",
    "def split_and_encode(text_list):\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "    split(text_list),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# 展示编码结果\n",
    "inputs_test = split_and_encode(text)\n",
    "for key,value in inputs_test.items():\n",
    "    print(key ,\": \\n\", value, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、定义新的Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "# Dataset类，用于处理训练集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        # 用pandas读取csv，并将其转换为Dataset\n",
    "        data_df = pd.read_csv(path)\n",
    "        data_list = data_df.to_dict(orient=\"list\")\n",
    "        dataset = datasets.Dataset.from_dict(data_list)\n",
    "\n",
    "        #过滤掉太长的句子\n",
    "        def f(data):\n",
    "            return data['length'] <= 200 - 2\n",
    "\n",
    "        dataset = dataset.filter(f)\n",
    "\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        token = split_sent(self.data[index]['text'])\n",
    "        label = eval(self.data[index]['labels'])\n",
    "\n",
    "        return token, label \n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = Dataset('data/sep_by_。.csv')\n",
    "\n",
    "# 使用数据集\n",
    "token, label = dataset[0]\n",
    "\n",
    "len(dataset), token, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、定义数据整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据整理函数\n",
    "def collate_fn(data):\n",
    "    \n",
    "    # tokens是分解后的文本\n",
    "    tokens = [row[0] for row in data]\n",
    "    # labels是对应的标记\n",
    "    labels = [row[1] for row in data]\n",
    "\n",
    "    # inputs是tokens的编码\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True)\n",
    "\n",
    "    # 编码的长度（最长长度）\n",
    "    length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 将[CLS]和[PAD]标记为3，并将标记长度与编码长度统一\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [3] + labels[i]\n",
    "        labels[i] += [3] * length\n",
    "        labels[i] = labels[i][:length]\n",
    "\n",
    "    return inputs, torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、定义数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "#查看数据样例\n",
    "for i, (inputs, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(len(labels))\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "print(labels)\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k,'\\t', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = AutoModel.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000, '万')\n",
    "\n",
    "#模型试算\n",
    "pretrained(**inputs).last_hidden_state.shape\n",
    "\n",
    "torch.save(pretrained, 'model/pretrained.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、定义下游主体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用可用的第一个 GPU\n",
    "device1 = torch.device(\"cpu\")\n",
    "\n",
    "# pretrained = torch.load('model/pretrained.model')\n",
    "\n",
    "#定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        self.rnn = torch.nn.GRU(768, 768,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(768, 8)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc(out).softmax(dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "pretrained = pretrained.to(device)\n",
    "model = Model()\n",
    "inputs = inputs.to(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对计算结果和label变形,并且移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    #变形,便于计算loss\n",
    "    #[b, lens, 8] -> [b*lens, 8]\n",
    "    outs = outs.reshape(-1, 8)\n",
    "    #[b, lens] -> [b*lens]\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    #[b, lens] -> [b*lens - pad]\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2, 3, 8), torch.ones(2, 3),\n",
    "                       torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取正确数量和总数\n",
    "def get_correct_and_total_count(labels, outs):\n",
    "    #[b*lens, 8] -> [b*lens]\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    #计算除了0以外元素的正确率,因为0太多了,包括的话,正确率很容易虚高\n",
    "    select = labels != 0\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = []\n",
    "# for step, (inputs, labels) in enumerate(loader):\n",
    "#     li.append((step, inputs, labels))\n",
    "# torch.save(li, 'tensor_file.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_tensor = torch.load('tensor_file.pt')\n",
    "\n",
    "\n",
    "# print(len(loaded_tensor[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "#训练\n",
    "def train(epochs):\n",
    "    lr = 2e-5 if model.tuneing else 5e-4\n",
    "\n",
    "    #训练\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "            #模型计算\n",
    "            #[b, lens] -> [b, lens, 8]\n",
    "            inputs = inputs.to(device)\n",
    "            outs = model(inputs)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #对outs和label变形,并且移除pad\n",
    "            #outs -> [b, lens, 8] -> [c, 8]\n",
    "            #labels -> [b, lens] -> [c]\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                                  inputs['attention_mask'])\n",
    "\n",
    "            #梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "                accuracy = counts[0] / counts[1]\n",
    "                accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "                print(epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "\n",
    "        torch.save(model, 'model/model.model')\n",
    "\n",
    "model.fine_tuneing(False)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device1 = torch.device(\"cpu\")\n",
    "\n",
    "#测试\n",
    "def test():\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                              batch_size=128,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "        if step == 5:\n",
    "            break\n",
    "        print(step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "            inputs = inputs.to(device1)\n",
    "            model_load = model_load.to(device1)\n",
    "            outs = model_load(inputs)\n",
    "\n",
    "        #对outs和label变形,并且移除pad\n",
    "        #outs -> [b, lens, 8] -> [c, 8]\n",
    "        #labels -> [b, lens] -> [c]\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                              inputs['attention_mask'])\n",
    "\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print(correct / total, correct_content / total_content)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def predict_one():\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    text = [\"红土创新基金管理有限公司6月30日发布公告，红土创新盐田港REIT(180301)的基金经理新聘陈超。\"\n",
    "    ,\"金能科技: 金能科技股份有限公司关于“金能转债”转股价格调整的提示性公告\",\n",
    "    \"大地熊成功收购技研株式会社安徽大地熊新材料股份有限公司2023年6月22日,大地熊日本株式会社与技研株式会社(英文名:p.m.giken inc,简称pm公司)原有股东和相关方签订收购合同》,大地熊日本株式会社全资收购pm公司并即时启动相关手续交接。 pm公司是专业从事磁钢整体磁气回路的解析与设计,注塑磁体的开发、制造与销售。此次收购是大地熊向注塑磁体领域的全新产业布局,将进一步推进大地熊海内外战略合作与业务拓展,丰富公司磁性材料及部件产品种类,提升公司综合实力和整体竞争力。\",\n",
    "    \"秋田满满、小鹿蓝蓝食安问题频发,投诉也没用随着我国新生代父母的科学喂养观念加强,我国婴童辅食行业消费规模持续上升。早在2019年我国婴幼儿辅食消费市场规模就已经达到404亿元,年复合增长率高达23%,预计未来我国婴幼儿辅食市场规模应在千亿以上。\"]\n",
    "    \n",
    "    text_split = []\n",
    "    for sent in text:\n",
    "        sent_split = [char for char in sent]\n",
    "        text_split.append(sent_split)\n",
    "\n",
    "    #print(text_split)\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "    text_split,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "        #label = labels[i, select]\n",
    "        \n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''), len(input_id))\n",
    "\n",
    "        #输出tag\n",
    "        for tag in [out]:\n",
    "            s = ''\n",
    "            for j in range(len(tag)):\n",
    "                if tag[j] == 0:\n",
    "                    s += '·'\n",
    "                    continue\n",
    "                s += tokenizer.decode(input_id[j])\n",
    "                s += str(tag[j].item())\n",
    "\n",
    "            print(s)\n",
    "        print('==========================')\n",
    "    \n",
    "predict_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
