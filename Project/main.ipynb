{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据集预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、将训练集的mentions转换为text对应的标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取训练集\n",
    "df = pd.read_excel(\"data/train.xlsx\")\n",
    "\n",
    "# entity的起始和后续标记\n",
    "B_ENT = 3\n",
    "I_ENT = 4\n",
    "\n",
    "# 逐个样本处理\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # mentions的列表\n",
    "    mention_list = eval(row['mentions'])\n",
    "\n",
    "    # text字符串的长度\n",
    "    text_len = len(row['text'])\n",
    "\n",
    "    # 创建与text等长的label\n",
    "    label = [0]*text_len\n",
    "\n",
    "    # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "    for mention in mention_list:\n",
    "\n",
    "        # mention字典中的'offset'字段表示某个实体的开始和结束位置，左闭右开\n",
    "        (start, end) = eval(mention['offset'])\n",
    "\n",
    "        # entity的起始位置标记为B_ENT\n",
    "        label[start] = B_ENT\n",
    "        \n",
    "        # entity的其余位置标记为I_ENT\n",
    "        for num in range(start+1, end):\n",
    "            label[num] = I_ENT\n",
    "\n",
    "    # 存储label\n",
    "    df.loc[index, \"labels\"] = str(label)\n",
    "    \n",
    "# 保存为新的CSV文件\n",
    "df.to_csv(\"data/train_labeled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、分句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案一：用句号划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = pd.read_csv('data/train_labeled.csv')\n",
    "\n",
    "# # 以句号为分隔\n",
    "# char_sep = '。'\n",
    "# li = []\n",
    "\n",
    "# for index, row in df0.iterrows():\n",
    "#     # indices是分隔的位置list，第一句从头开始\n",
    "#     indices = [-1]\n",
    "\n",
    "#     text = row['text']\n",
    "#     text_len = len(text)\n",
    "\n",
    "#     # 找到所有的句号\n",
    "#     for i in range(len(text)):\n",
    "#         if text[i] == char_sep:\n",
    "#             indices.append(i)\n",
    "\n",
    "#     # 如果结尾不是句号，添加最后一个字符为分隔\n",
    "#     if indices[-1] != text_len - 1:\n",
    "#         indices.append(text_len - 1)\n",
    "\n",
    "#     # 逐句处理\n",
    "#     for j in range(len(indices)-1):\n",
    "#         begin = indices[j] + 1\n",
    "#         end = indices[j+1] + 1\n",
    "#         sent = text[begin: end]\n",
    "#         label = eval(row['labels'])[begin: end]\n",
    "#         length = end - begin\n",
    "#         li.append([length, sent, label])\n",
    "    \n",
    "# df = pd.DataFrame(li, columns = ['length', 'text', 'labels'])\n",
    "\n",
    "# df.to_csv('data/sep_by_。.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二、用多种标点划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('data/train_labeled.csv')\n",
    "\n",
    "max_size = 100\n",
    "\n",
    "# 以句号为分隔\n",
    "char_sep = '。'\n",
    "sep_list1 = [',',' ',';']\n",
    "sep_list2 = ['、',':']\n",
    "li = []\n",
    "\n",
    "for index, row in df0.iterrows():\n",
    "    # indices是分隔的位置list，第一句从头开始\n",
    "    indices = [-1]\n",
    "\n",
    "    text = row['text']\n",
    "    text_len = len(text)\n",
    "\n",
    "    # 找到所有的句号\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == char_sep:\n",
    "            indices.append(i)\n",
    "        else: \n",
    "            if i - indices[-1] >= max_size - 2:\n",
    "                for j in range (i, indices[-1], -1):\n",
    "                    if text[j] in sep_list1:\n",
    "                        indices.append(j)\n",
    "                        break\n",
    "                    else:\n",
    "                        if text[j] in sep_list2:\n",
    "                            indices.append(j)\n",
    "                            break\n",
    "        \n",
    "\n",
    "    # 如果结尾不是句号，添加最后一个字符为分隔\n",
    "    if indices[-1] != text_len - 1:\n",
    "        indices.append(text_len - 1)\n",
    "\n",
    "    # 逐句处理\n",
    "    for j in range(len(indices)-1):\n",
    "        begin = indices[j] + 1\n",
    "        end = indices[j+1] + 1\n",
    "        sent = text[begin: end]\n",
    "        label = eval(row['labels'])[begin: end]\n",
    "        length = end - begin\n",
    "        if length >= 10:\n",
    "            li.append([length, sent, label])\n",
    "    \n",
    "df = pd.DataFrame(li, columns = ['length', 'text', 'labels'])\n",
    "\n",
    "df = df.sort_values(by='length', ascending=False)\n",
    "\n",
    "df.to_csv('data/sep_'+str(max_size)+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案二、指定最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/train_labeled.csv')\n",
    "# df_s = pd.DataFrame(columns = ['segment_number','id','segment_len','text','mentions','labels'])\n",
    "# max_size = 100\n",
    "# #句段长度上限\n",
    "# #print(df_s.shape)\n",
    "# #header = 0 把第一行作为列名称，首位空缺设置为默认的0\n",
    "# #print(df.loc[0])\n",
    "# #显示列标\n",
    "# #预期效果：每行是一个短于max_size样本片段，一个样本占有若干行（若干片段）。\n",
    "# #预期效果：每列是一个片段的信息：\n",
    "# #         列标：0,               1，        2，      3，      4，          5，          \n",
    "# #              所属样本第几个片段 所属样本id 片段长度 片段文本  片段mentions  片段labels\n",
    "# seg_num = 0\n",
    "# #总片段下标\n",
    "\n",
    "# for row in df.values:\n",
    "#     #print(type(row),row.shape)\n",
    "#     mention_list = eval(row[3])\n",
    "#     #print(eval(row[5]))\n",
    "#     #al_len = 0\n",
    "#     al_seg_len = 0\n",
    "#     segs = 0\n",
    "#     #该文本已经切割的片段总长度和片段数目\n",
    "    \n",
    "#     df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "#     #df_s = df_s.append(pd.DataFrame([[segs,row[1],0,'',[],'']],columns = df_s.columns))\n",
    "#     #print(type(df_s.loc[seg_num]))\n",
    "#     #初始化该文本第一个片段\n",
    "#     for i in range(len(mention_list)):\n",
    "#     # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "        \n",
    "#         start, end = eval(mention_list[i]['offset'])\n",
    "#         # mention字典中的'offset'字段表示某个实体的开始和结束位置，左闭右开\n",
    "#         if end < al_seg_len + max_size:\n",
    "#         #实体全部在长度范围内\n",
    "#             df_s.iloc[seg_num,4].append(mention_list[i])\n",
    "#             #print(mention_list[i])\n",
    "#             if i == len(mention_list):\n",
    "#             #最后一个mention\n",
    "#                 df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,len(row[2]))]\n",
    "#                 df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,len(eval(row[5])))])\n",
    "#                 al_seg_len = min(al_seg_len + max_size,len(row[2]))\n",
    "\n",
    "#                 df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "#                 seg_num +=1   \n",
    "#                 segs += 1\n",
    "#         else:\n",
    "#         #过长，该实体放入下一个片段,开始整理已经确定的片段\n",
    "#             i -= 1\n",
    "#             #下一次还要放入该mention\n",
    "#             df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,start)]\n",
    "#             df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,start)])\n",
    "            \n",
    "#             al_seg_len = min(al_seg_len + max_size,start)\n",
    "\n",
    "#             df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "#             seg_num +=1   \n",
    "#             segs += 1\n",
    "#             if al_seg_len < len(row[2]): df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "#             #新片段\n",
    "#     #最后处理尾部无mentions区域\n",
    "#     while al_seg_len < len(row[2]):\n",
    "#         df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,len(row[2]))]\n",
    "#         df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,len(eval(row[5])))])\n",
    "#         al_seg_len = min(al_seg_len + max_size,len(row[2]))\n",
    "#         if len(eval(df_s.iloc[seg_num,5])) != len(df_s.iloc[seg_num,3]) or len(eval(df_s.iloc[seg_num,5]))>max_size: \n",
    "#            print(len(df_s.iloc[seg_num,3]),len(eval(df_s.iloc[seg_num,5])))\n",
    "#         df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "#         seg_num +=1   \n",
    "#         segs += 1\n",
    "#         if al_seg_len < len(row[2]): df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "#     #print(df_s.loc[0])\n",
    "\n",
    "# df_s.to_csv(\"data/train_separated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载分词器并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : \n",
      " tensor([[ 101, 5273, 1759, 1158, 3173, 1825, 7032, 5052, 4415, 3300, 7361, 1062,\n",
      "         1385,  127, 3299,  124,  121, 3189, 1355, 2357, 1062, 1440, 8024, 5273,\n",
      "         1759, 1158, 3173, 4663, 4506, 3949,  160,  147,  151,  162,  113,  122,\n",
      "          129,  121,  124,  121,  122,  114, 4638, 1825, 7032, 5307, 4415, 3173,\n",
      "         5470, 7357, 6631,  511,  102],\n",
      "        [ 101, 7032, 5543, 4906, 2825,  131, 7032, 5543, 4906, 2825, 5500,  819,\n",
      "         3300, 7361, 1062, 1385, 1068,  754,  100, 7032, 5543, 6760,  965,  100,\n",
      "         6760, 5500,  817, 3419, 6444, 3146, 4638, 2990, 4850, 2595, 1062, 1440,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0]]) \n",
      "\n",
      "token_type_ids : \n",
      " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]) \n",
      "\n",
      "attention_mask : \n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 测试文本\n",
    "text = [\"红土创新基金管理有限公司6月30日发布公告，红土创新盐田港REIT(180301)的基金经理新聘陈超。\"\n",
    ",\"金能科技: 金能科技股份有限公司关于“金能转债”转股价格调整的提示性公告\"]\n",
    "\n",
    "# 将文本进行逐字分解\n",
    "def split_sent(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "def split(text_list):\n",
    "\n",
    "    text_split = []\n",
    "    for sent in text_list:\n",
    "        sent_split = split_sent(sent)\n",
    "        text_split.append(sent_split)\n",
    "    return text_split\n",
    "\n",
    "\n",
    "# 分解并编码\n",
    "def split_and_encode(text_list):\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "    split(text_list),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# 展示编码结果\n",
    "inputs_test = split_and_encode(text)\n",
    "for key,value in inputs_test.items():\n",
    "    print(key ,\": \\n\", value, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、定义新的Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6434/6434 [00:00<00:00, 183786.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "# Dataset类，用于处理训练集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        # 用pandas读取csv，并将其转换为Dataset\n",
    "        data_df = pd.read_csv(path)\n",
    "        data_list = data_df.to_dict(orient=\"list\")\n",
    "        dataset = datasets.Dataset.from_dict(data_list)\n",
    "\n",
    "        #过滤掉太长的句子\n",
    "        def f(data):\n",
    "            return data['length'] <= max_size - 2\n",
    "\n",
    "        dataset = dataset.filter(f)\n",
    "\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        token = split_sent(self.data[index]['text'])\n",
    "        label = eval(self.data[index]['labels'])\n",
    "\n",
    "        return token, label \n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = Dataset('data/sep_'+str(max_size)+'.csv')\n",
    "\n",
    "# 使用数据集\n",
    "token, label = dataset[0]\n",
    "\n",
    "#len(dataset), list(token), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、定义数据整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据整理函数\n",
    "def collate_fn(data):\n",
    "    \n",
    "    # tokens是分解后的文本\n",
    "    tokens = [row[0] for row in data]\n",
    "    # labels是对应的标记\n",
    "    labels = [row[1] for row in data]\n",
    "\n",
    "    # inputs是tokens的编码\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True)\n",
    "\n",
    "    # 编码的长度（最长长度）\n",
    "    length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 将[CLS]和[PAD]标记为3，并将标记长度与编码长度统一\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [3] + labels[i]\n",
    "        labels[i] += [3] * length\n",
    "        labels[i] = labels[i][:length]\n",
    "\n",
    "    return inputs, torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、定义数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "20\n",
      "[CLS] 融 资 余 额 占 比 前 十 的 个 股 平 均 流 通 市 值 为 4 1. 9 2 亿 元, 融 资 余 额 占 比 最 高 的 仁 东 控 股 最 新 流 通 市 值 为 2 8. 0 0 亿 元 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([[3, 0, 0,  ..., 3, 3, 3],\n",
      "        [3, 0, 3,  ..., 3, 3, 3],\n",
      "        [3, 0, 3,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [3, 0, 0,  ..., 3, 3, 3],\n",
      "        [3, 0, 0,  ..., 3, 3, 3],\n",
      "        [3, 0, 0,  ..., 3, 3, 3]])\n",
      "input_ids \t torch.Size([20, 98])\n",
      "token_type_ids \t torch.Size([20, 98])\n",
      "attention_mask \t torch.Size([20, 98])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "#查看数据样例\n",
    "for i, (inputs, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(len(labels))\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "print(labels)\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k,'\\t', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5974.0416 万\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = AutoModel.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000, '万')\n",
    "\n",
    "#模型试算\n",
    "pretrained(**inputs).last_hidden_state.shape\n",
    "\n",
    "torch.save(pretrained, 'model/pretrained.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、定义下游主体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 98, 8])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用可用的第一个 GPU\n",
    "device1 = torch.device(\"cpu\")\n",
    "\n",
    "# pretrained = torch.load('model/pretrained.model')\n",
    "\n",
    "#定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        self.rnn = torch.nn.GRU(768, 768,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(768, 8)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc(out).softmax(dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "pretrained = pretrained.to(device)\n",
    "#model = torch.load('model/old.model')\n",
    "model = Model()\n",
    "inputs = inputs.to(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4701,  1.0503,  0.0942, -2.0883, -0.0691,  0.4383,  0.9786, -1.3162],\n",
       "         [-0.2091, -0.2392, -0.2887, -0.6284, -0.4473, -0.1226,  0.3278, -1.1443],\n",
       "         [-0.2640, -0.9409,  0.6567, -1.3145, -0.4140, -0.2321,  1.4232, -0.5155],\n",
       "         [-1.6754, -0.4503,  0.5053, -0.2655,  0.9738,  0.7809, -0.3302,  0.2328],\n",
       "         [ 0.4400, -0.2033,  0.6827,  2.2053, -1.7541,  0.8680,  0.0548,  0.2773],\n",
       "         [-1.9146, -0.9129, -0.7194, -0.1552, -0.3077,  0.5339, -1.5717, -0.4352]]),\n",
       " tensor([1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对计算结果和label变形,并且移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    #变形,便于计算loss\n",
    "    #[b, lens, 8] -> [b*lens, 8]\n",
    "    outs = outs.reshape(-1, 8)\n",
    "    #[b, lens] -> [b*lens]\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    #[b, lens] -> [b*lens - pad]\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2, 3, 8), torch.ones(2, 3),\n",
    "                       torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16, 0, 0)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取正确数量和总数\n",
    "def get_correct_and_total_count(labels, outs):\n",
    "    #[b*lens, 8] -> [b*lens]\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    #计算除了0以外元素的正确率,因为0太多了,包括的话,正确率很容易虚高\n",
    "    select = (labels == 3) + (labels == 4)\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354.9704\n",
      "0 0 2.0862789154052734 0.02663622526636225 0.08695652173913043\n",
      "0 50 1.3383339643478394 0.936267071320182 0.19230769230769232\n",
      "0 100 1.3648669719696045 0.9092284417549168 0.14285714285714285\n",
      "0 150 1.3306382894515991 0.9434129089301503 0.23809523809523808\n",
      "0 200 1.3988412618637085 0.8751962323390895 0.11173184357541899\n",
      "0 250 1.3660998344421387 0.9079307201458523 0.1652892561983471\n",
      "0 300 1.3919060230255127 0.882120253164557 0.11834319526627218\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "#训练\n",
    "def train(epochs):\n",
    "    lr = 2e-5 if model.tuneing else 5e-4\n",
    "\n",
    "    #训练\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "            #模型计算\n",
    "            #[b, lens] -> [b, lens, 8]\n",
    "            inputs = inputs.to(device)\n",
    "            outs = model(inputs)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #对outs和label变形,并且移除pad\n",
    "            #outs -> [b, lens, 8] -> [c, 8]\n",
    "            #labels -> [b, lens] -> [c]\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                                  inputs['attention_mask'])\n",
    "\n",
    "            #梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "                accuracy = counts[0] / counts[1]\n",
    "                accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "                print(epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "\n",
    "        if accuracy_content > 0.8:\n",
    "            break\n",
    "        torch.save(model, 'model/model.model')\n",
    "\n",
    "model.fine_tuneing(False)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6329.012\n",
      "0 0 1.3921314477920532 0.881896551724138 0.12738853503184713\n",
      "0 50 1.349989891052246 0.9240196078431373 0.17699115044247787\n",
      "0 100 1.392376184463501 0.8816326530612245 0.14705882352941177\n",
      "0 150 1.3194451332092285 0.9551166965888689 0.45977011494252873\n",
      "0 200 1.323422908782959 0.950834879406308 0.43010752688172044\n",
      "0 250 1.331437110900879 0.942630185348632 0.39622641509433965\n",
      "0 300 1.3493136167526245 0.9247038917089678 0.304\n",
      "1 0 1.3315593004226685 0.94240317775571 0.422680412371134\n",
      "1 50 1.3123054504394531 0.9624542124542125 0.5119047619047619\n",
      "1 100 1.3029303550720215 0.9708654670094259 0.575\n",
      "1 150 1.352297306060791 0.9213085764809903 0.37681159420289856\n",
      "1 200 1.3433125019073486 0.9307992202729045 0.4017094017094017\n",
      "1 250 1.3939942121505737 0.8797763280521901 0.3128491620111732\n",
      "1 300 1.32818603515625 0.9459962756052142 0.45263157894736844\n",
      "2 0 1.3529618978500366 0.920675105485232 0.36363636363636365\n",
      "2 50 1.378406286239624 0.8951160928742994 0.3021978021978022\n",
      "2 100 1.3361860513687134 0.9370689655172414 0.43333333333333335\n",
      "2 150 1.3784918785095215 0.8940677966101694 0.2916666666666667\n",
      "2 200 1.3508630990982056 0.9225978647686833 0.35658914728682173\n",
      "2 250 1.3083312511444092 0.964041095890411 0.5555555555555556\n",
      "2 300 1.3115744590759277 0.9613050075872535 0.4772727272727273\n",
      "3 0 1.3498107194900513 0.922874671340929 0.3582089552238806\n",
      "3 50 1.3006848096847534 0.9725228975853455 0.5972222222222222\n",
      "3 100 1.3247419595718384 0.9474226804123711 0.5283018867924528\n",
      "3 150 1.3131123781204224 0.9602169981916817 0.5393258426966292\n",
      "3 200 1.2971816062927246 0.975925925925926 0.6515151515151515\n",
      "3 250 1.3365873098373413 0.9354120267260579 0.4766355140186916\n",
      "3 300 1.299041986465454 0.9742063492063492 0.6388888888888888\n",
      "4 0 1.3297353982925415 0.9426229508196722 0.4954954954954955\n",
      "4 50 1.3234539031982422 0.9496268656716418 0.47959183673469385\n",
      "4 100 1.3527204990386963 0.9189640768588136 0.3821656050955414\n",
      "4 150 1.3006983995437622 0.9728937728937729 0.5512820512820513\n",
      "4 200 1.3318536281585693 0.94106463878327 0.49056603773584906\n",
      "4 250 1.3379172086715698 0.9345132743362832 0.4296875\n",
      "4 300 1.3648439645767212 0.9072356215213359 0.36129032258064514\n",
      "5 0 1.306127905845642 0.9671497584541063 0.569620253164557\n",
      "5 50 1.3156026601791382 0.957391304347826 0.5154639175257731\n",
      "5 100 1.3319590091705322 0.9423942394239424 0.46788990825688076\n",
      "5 150 1.3678876161575317 0.9027504911591355 0.35714285714285715\n",
      "5 200 1.3270853757858276 0.9448275862068966 0.48695652173913045\n",
      "5 250 1.3229293823242188 0.9498464687819856 0.5164835164835165\n",
      "5 300 1.3393034934997559 0.9179869524697111 0.4041095890410959\n",
      "6 0 1.2965037822723389 0.9775036284470247 0.7913043478260869\n",
      "6 50 1.2901159524917603 0.9844115354637568 0.8913043478260869\n",
      "6 100 1.298886775970459 0.9751332149200711 0.801980198019802\n",
      "6 150 1.287169337272644 0.9868995633187773 0.8529411764705882\n",
      "6 200 1.2894703149795532 0.9839357429718876 0.860655737704918\n",
      "6 250 1.2827560901641846 0.9919499105545617 0.9493670886075949\n",
      "6 300 1.301595687866211 0.972318339100346 0.7835820895522388\n",
      "7 0 1.3191372156143188 0.95483288166215 0.9484536082474226\n",
      "7 50 1.3002305030822754 0.97288842544317 0.7831325301204819\n",
      "7 100 1.2791602611541748 0.995049504950495 0.9818181818181818\n",
      "7 150 1.296215295791626 0.9778801843317972 0.7843137254901961\n",
      "7 200 1.2950254678726196 0.9796264855687606 0.8690476190476191\n",
      "7 250 1.2756576538085938 0.9981834695731153 1.0\n",
      "7 300 1.2858024835586548 0.9882352941176471 0.8522727272727273\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.9824910354101299 0.9103922129974235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#测试\n",
    "def test():\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                              batch_size=128,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "        if step == 5:\n",
    "            break\n",
    "        print(step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "            inputs = inputs.to(device)\n",
    "            model_load = model_load.to(device)\n",
    "            outs = model_load(inputs)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        #对outs和label变形,并且移除pad\n",
    "        #outs -> [b, lens, 8] -> [c, 8]\n",
    "        #labels -> [b, lens] -> [c]\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                              inputs['attention_mask'])\n",
    "\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print(correct / total, correct_content / total_content)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]红土创新基金管理有限公司6月30日发布公告，红土创新盐田港reit(180301)的基金经理新聘陈超。[SEP] 53\n",
      "[CLS]3红3土4创4新4基4金4管4理4有4限4公4司4··········红3土4创4新4盐4田4港4r4e4i4t4··················[SEP]3\n",
      "==========================\n",
      "[CLS]金能科技:金能科技股份有限公司关于[UNK]金能转债[UNK]转股价格调整的提示性公告[SEP] 37\n",
      "[CLS]3金3能4科4技4·金3能4科4技4股4份4有4限4公4司4····················[SEP]3\n",
      "==========================\n",
      "[CLS]大地熊成功收购技研株式会社安徽大地熊新材料股份有限公司2023年6月22日,大地熊日本株式会社与技研株式会社(英文名:p.m.gikeninc,简称pm公司)原有股东和相关方签订收购合同》,大地熊日本株式会社全资收购pm公司并即时启动相关手续交接。pm公司是专业从事磁钢整体磁气回路的解析与设计,注塑磁体的开发、制造与销售。此次收购是大地熊向注塑磁体领域的全新产业布局,将进一步推进大地熊海内外战略合作与业务拓展,丰富公司磁性材料及部件产品种类,提升公司综合实力和整体竞争力。[SEP] 240\n",
      "[CLS]3大3地4熊4····技3研4株4式4会4社4安4徽4大4地4熊4新4材4料4股4份4有4限4公4司4···········大3地4熊4日4本4株4式4会4社4·技3研4株4式4会4社4·····p3.4m4.4g4i4k4e4n4i4n4c4···p3m4公4司4·················大3地4熊4日4本4株4式4会4社4····p3m4公4司4············p3m4公4司4·······································大3地4熊4·····················大3地4熊4············································[SEP]3\n",
      "==========================\n",
      "[CLS]黑猫股份:设立全资子公司投建年产16万吨碳材/橡胶复合母胶项目证券时报e公司讯,黑猫股份(002068)6月27日晚间公告,公司拟出资1亿元在辽宁省朝阳市高新技术产业开发区设立全资子公司辽宁黑猫复合新材料科技有限公司(简称辽宁黑猫),并且将以辽宁黑猫为项目主体,投资新建年产16万吨碳材/橡胶复合母胶项目,分三期进行建设,项目预计投资总额为6.88亿元。[SEP] 179\n",
      "[CLS]3黑3猫4股4份4····································黑3猫4股4份4·················································辽3宁4黑4猫4复4合4新4材4料4科4技4有4限4公4司4···辽3宁4黑4猫4······辽3宁4黑4猫4····················································[SEP]3\n",
      "==========================\n",
      "[CLS]秋田满满、小鹿蓝蓝食安问题频发,投诉也没用随着我国新生代父母的科学喂养观念加强,我国婴童辅食行业消费规模持续上升。早在2019年我国婴幼儿辅食消费市场规模就已经达到404亿元,年复合增长率高达23%,预计未来我国婴幼儿辅食市场规模应在千亿以上。[SEP] 124\n",
      "[CLS]3秋3田4满4满4·小3鹿4···················································································································[SEP]3\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def predict_one():\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.to(device1)\n",
    "    model_load.eval()\n",
    "\n",
    "    text = [\"红土创新基金管理有限公司6月30日发布公告，红土创新盐田港REIT(180301)的基金经理新聘陈超。\"\n",
    "    ,\"金能科技: 金能科技股份有限公司关于“金能转债”转股价格调整的提示性公告\",\n",
    "    \"大地熊成功收购技研株式会社安徽大地熊新材料股份有限公司2023年6月22日,大地熊日本株式会社与技研株式会社\\\\\n",
    "        (英文名:p.m.giken inc,简称pm公司)原有股东和相关方签订收购合同》,大地熊日本株式会社全资收购pm公司\\\\\n",
    "        并即时启动相关手续交接。 pm公司是专业从事磁钢整体磁气回路的解析与设计,注塑磁体的开发、制造与销售。\\\\\n",
    "        此次收购是大地熊向注塑磁体领域的全新产业布局,将进一步推进大地熊海内外战略合作与业务拓展,丰富公司磁性材料及部件产品种类,提升公司综合实力和整体竞争力。\",\n",
    "    \"黑猫股份:设立全资子公司投建年产16万吨碳材/橡胶复合母胶项目证券时报e公司讯,黑猫股份(002068)6月27日晚间\\\\\n",
    "    公告,公司拟出资1亿元在辽宁省朝阳市高新技术产业开发区设立全资子公司辽宁黑猫复合新材料科技有限公司(简称辽\\\\\n",
    "        宁黑猫),并且将以辽宁黑猫为项目主体,投资新建年产16万吨碳材/橡胶复合母胶项目,分三期进行建设,项目预计投资总额为6.88亿元。\",\n",
    "    \"秋田满满、小鹿蓝蓝食安问题频发,投诉也没用随着我国新生代父母的科学喂养观念加强,我国婴童辅食行业消费规模\\\\\n",
    "        持续上升。早在2019年我国婴幼儿辅食消费市场规模就已经达到404亿元,年复合增长率高达23%,预计未来我国婴幼儿辅食市场规模应在千亿以上。\"]\n",
    "    \n",
    "    text_split = []\n",
    "    for sent in text:\n",
    "        sent_split = [char for char in sent]\n",
    "        text_split.append(sent_split)\n",
    "\n",
    "    #print(text_split)\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "    text_split,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "        #label = labels[i, select]\n",
    "        \n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''), len(input_id))\n",
    "\n",
    "        #输出tag\n",
    "        for tag in [out]:\n",
    "            s = ''\n",
    "            for j in range(len(tag)):\n",
    "                if tag[j] == 0:\n",
    "                    s += '·'\n",
    "                    continue\n",
    "                s += tokenizer.decode(input_id[j])\n",
    "                s += str(tag[j].item())\n",
    "\n",
    "            print(s)\n",
    "        print('==========================')\n",
    "    \n",
    "predict_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "df = pd.read_excel('data/train.xlsx')\n",
    "kb = pd.read_excel('data/knowledge_base_new.xlsx')\n",
    "\n",
    "\n",
    "def find(entity, offset):\n",
    "    for index, row in kb.iterrows():\n",
    "        if index not in index_list and (row['stockName'] == entity or row['companyName'] == entity):\n",
    "            know = \\\n",
    "            {\"companyId\": row['companyId'],\n",
    "            \"companyName\": row['companyName'],\n",
    "            \"mention\": entity, \n",
    "            \"offset\": offset, \n",
    "            \"stockId\": int(row['stockId']) if not math.isnan(row['stockId']) else 'null', \n",
    "            \"stockName\": row['stockName'] if not math.isnan(row['stockId']) else 'null', \n",
    "            \"type\": row['type'][:2]}\n",
    "            knowledge_list.append(know)\n",
    "            index_list.append(index)\n",
    "        \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    print(index)\n",
    "\n",
    "    mention_list = eval(row['mentions'])\n",
    "    entity_list = []\n",
    "    index_list = []\n",
    "    knowledge_list = []\n",
    "\n",
    "    # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "    for mention in mention_list:\n",
    "\n",
    "        entity = mention['mention']\n",
    "\n",
    "        offset = mention['offset']\n",
    "\n",
    "        if entity not in entity_list:\n",
    "\n",
    "            find(entity, offset)\n",
    "\n",
    "            entity_list.append(entity)\n",
    "\n",
    "    # 存储结果\n",
    "    df.loc[index, \"result\"] = str(knowledge_list)\n",
    "\n",
    "df.to_csv('data/knowledge.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
