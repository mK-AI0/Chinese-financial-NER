{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 从 JSON 文件中读取常量数据\n",
    "with open('constants.json', 'r') as json_file:\n",
    "    constants = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据集预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、标记\n",
    "将训练集的mentions转换为text对应的标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取训练集\n",
    "df = pd.read_excel(\"data/train.xlsx\")\n",
    "\n",
    "# entity的起始和后续标记\n",
    "B_ENT = constants[\"B_ENT\"]\n",
    "I_ENT = constants[\"I_ENT\"]\n",
    "\n",
    "# 逐个样本处理\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # mentions的列表\n",
    "    mention_list = eval(row['mentions'])\n",
    "\n",
    "    # text字符串的长度\n",
    "    text_len = len(row['text'])\n",
    "\n",
    "    # 创建与text等长的label\n",
    "    label = [0]*text_len\n",
    "\n",
    "    # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "    for mention in mention_list:\n",
    "\n",
    "        # mention字典中的'offset'字段表示某个实体的开始和结束位置，左闭右开\n",
    "        (start, end) = eval(mention['offset'])\n",
    "\n",
    "        # entity的起始位置标记为B_ENT\n",
    "        label[start] = B_ENT\n",
    "        \n",
    "        # entity的其余位置标记为I_ENT\n",
    "        for num in range(start+1, end):\n",
    "            label[num] = I_ENT\n",
    "\n",
    "    # 存储label\n",
    "    df.loc[index, \"labels\"] = str(label)\n",
    "    \n",
    "# 保存为新的CSV文件\n",
    "df.to_csv(\"data/train_labeled.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、分句\n",
    "将大段新闻分割为短句，便于训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案一：用句号划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = pd.read_csv('data/train_labeled.csv')\n",
    "\n",
    "# # 以句号为分隔\n",
    "# char_sep = '。'\n",
    "# li = []\n",
    "\n",
    "# for index, row in df0.iterrows():\n",
    "#     # indices是分隔的位置list，第一句从头开始\n",
    "#     indices = [-1]\n",
    "\n",
    "#     text = row['text']\n",
    "#     text_len = len(text)\n",
    "\n",
    "#     # 找到所有的句号\n",
    "#     for i in range(len(text)):\n",
    "#         if text[i] == char_sep:\n",
    "#             indices.append(i)\n",
    "\n",
    "#     # 如果结尾不是句号，添加最后一个字符为分隔\n",
    "#     if indices[-1] != text_len - 1:\n",
    "#         indices.append(text_len - 1)\n",
    "\n",
    "#     # 逐句处理\n",
    "#     for j in range(len(indices)-1):\n",
    "#         begin = indices[j] + 1\n",
    "#         end = indices[j+1] + 1\n",
    "#         sent = text[begin: end]\n",
    "#         label = eval(row['labels'])[begin: end]\n",
    "#         length = end - begin\n",
    "#         li.append([length, sent, label])\n",
    "    \n",
    "# df = pd.DataFrame(li, columns = ['length', 'text', 'labels'])\n",
    "\n",
    "# df.to_csv('data/sep_by_。.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案二：指定最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/train_labeled.csv')\n",
    "# df_s = pd.DataFrame(columns = ['segment_number','id','segment_len','text','mentions','labels'])\n",
    "# max_size = 100\n",
    "# #句段长度上限\n",
    "# #print(df_s.shape)\n",
    "# #header = 0 把第一行作为列名称，首位空缺设置为默认的0\n",
    "# #print(df.loc[0])\n",
    "# #显示列标\n",
    "# #预期效果：每行是一个短于max_size样本片段，一个样本占有若干行（若干片段）。\n",
    "# #预期效果：每列是一个片段的信息：\n",
    "# #         列标：0,               1，        2，      3，      4，          5，          \n",
    "# #              所属样本第几个片段 所属样本id 片段长度 片段文本  片段mentions  片段labels\n",
    "# seg_num = 0\n",
    "# #总片段下标\n",
    "\n",
    "# for row in df.values:\n",
    "#     #print(type(row),row.shape)\n",
    "#     mention_list = eval(row[3])\n",
    "#     #print(eval(row[5]))\n",
    "#     #al_len = 0\n",
    "#     al_seg_len = 0\n",
    "#     segs = 0\n",
    "#     #该文本已经切割的片段总长度和片段数目\n",
    "    \n",
    "#     df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "#     #df_s = df_s.append(pd.DataFrame([[segs,row[1],0,'',[],'']],columns = df_s.columns))\n",
    "#     #print(type(df_s.loc[seg_num]))\n",
    "#     #初始化该文本第一个片段\n",
    "#     for i in range(len(mention_list)):\n",
    "#     # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "        \n",
    "#         start, end = eval(mention_list[i]['offset'])\n",
    "#         # mention字典中的'offset'字段表示某个实体的开始和结束位置，左闭右开\n",
    "#         if end < al_seg_len + max_size:\n",
    "#         #实体全部在长度范围内\n",
    "#             df_s.iloc[seg_num,4].append(mention_list[i])\n",
    "#             #print(mention_list[i])\n",
    "#             if i == len(mention_list):\n",
    "#             #最后一个mention\n",
    "#                 df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,len(row[2]))]\n",
    "#                 df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,len(eval(row[5])))])\n",
    "#                 al_seg_len = min(al_seg_len + max_size,len(row[2]))\n",
    "\n",
    "#                 df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "#                 seg_num +=1   \n",
    "#                 segs += 1\n",
    "#         else:\n",
    "#         #过长，该实体放入下一个片段,开始整理已经确定的片段\n",
    "#             i -= 1\n",
    "#             #下一次还要放入该mention\n",
    "#             df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,start)]\n",
    "#             df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,start)])\n",
    "            \n",
    "#             al_seg_len = min(al_seg_len + max_size,start)\n",
    "\n",
    "#             df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "#             seg_num +=1   \n",
    "#             segs += 1\n",
    "#             if al_seg_len < len(row[2]): df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "#             #新片段\n",
    "#     #最后处理尾部无mentions区域\n",
    "#     while al_seg_len < len(row[2]):\n",
    "#         df_s.iloc[seg_num,3]=row[2][al_seg_len:min(al_seg_len + max_size,len(row[2]))]\n",
    "#         df_s.iloc[seg_num,5]=str(eval(row[5])[al_seg_len:min(al_seg_len + max_size,len(eval(row[5])))])\n",
    "#         al_seg_len = min(al_seg_len + max_size,len(row[2]))\n",
    "#         if len(eval(df_s.iloc[seg_num,5])) != len(df_s.iloc[seg_num,3]) or len(eval(df_s.iloc[seg_num,5]))>max_size: \n",
    "#            print(len(df_s.iloc[seg_num,3]),len(eval(df_s.iloc[seg_num,5])))\n",
    "#         df_s.iloc[seg_num,2]=len(df_s.iloc[seg_num,3])\n",
    "#         seg_num +=1   \n",
    "#         segs += 1\n",
    "#         if al_seg_len < len(row[2]): df_s.loc[seg_num] = [segs,row[1],0,'',[],'']\n",
    "#     #print(df_s.loc[0])\n",
    "\n",
    "# df_s.to_csv(\"data/train_separated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方案三：用多种标点划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子的最大长度\n",
    "max_size = constants[\"max_size\"]\n",
    "\n",
    "# 以句号为分隔\n",
    "char_sep = '。'\n",
    "sep_list1 = [',',' ',';']\n",
    "sep_list2 = ['、',':',')']\n",
    "\n",
    "def find_sep(text):\n",
    "\n",
    "    \"\"\"找到所有划分位置\"\"\"\n",
    "\n",
    "    # 文本长度\n",
    "    text_len = len(text)\n",
    "\n",
    "    # indices是分隔的位置list，第一句从头开始\n",
    "    indices = [-1]\n",
    "\n",
    "    # 找到所有用于划分的标点符号\n",
    "    for i in range(text_len):\n",
    "\n",
    "        # 首先尝试找句号\n",
    "        if text[i] == char_sep:\n",
    "            indices.append(i)\n",
    "            continue\n",
    "\n",
    "        # 指定长度内没有句号\n",
    "        if i - indices[-1] >= max_size - 2:\n",
    "\n",
    "            # flag表示是否找到sep_list1中的标点\n",
    "            flag = 0\n",
    "\n",
    "            # 尝试找sep_list1中的标点\n",
    "            for j in range (i, indices[-1], -1):\n",
    "                if text[j] in sep_list1:\n",
    "                    indices.append(j)\n",
    "                    flag = 1\n",
    "                    break\n",
    "             \n",
    "            if flag == 0:\n",
    "\n",
    "                # 最后尝试找sep_list2中的标点\n",
    "                for j in range (i, indices[-1], -1):\n",
    "                    if text[j] in sep_list2:\n",
    "                        indices.append(j)\n",
    "                        break\n",
    "\n",
    "    # 如果结尾不是句号，添加最后一个字符为分隔\n",
    "    if indices[-1] != text_len - 1:\n",
    "        indices.append(text_len - 1)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def generate_data(indices):\n",
    "\n",
    "    \"\"\"逐句处理，将分割后的句子及标记存储至data\"\"\"\n",
    "\n",
    "    for j in range(len(indices)-1):\n",
    "\n",
    "        begin = indices[j] + 1\n",
    "        end = indices[j+1] + 1\n",
    "        length = end - begin\n",
    "\n",
    "        sent = text[begin: end]\n",
    "        sent = replace_space(sent)\n",
    "\n",
    "        label = eval(row['labels'])[begin: end]\n",
    "        \n",
    "        # 去除太短的句子\n",
    "        if length >= 10:\n",
    "            data.append([length, sent, label])\n",
    "\n",
    "\n",
    "def replace_space(sent):\n",
    "\n",
    "    \"\"\"由于tokenizer编码时会省略空格，因此将空格替换为其无法识别的省略号\"\"\"\n",
    "\n",
    "    return sent.replace(\" \", \"…\")\n",
    "\n",
    "\n",
    "# 带标记的数据\n",
    "df0 = pd.read_csv('data/train_labeled.csv')\n",
    "\n",
    "# 新的数据\n",
    "data = []\n",
    "\n",
    "# 逐行处理\n",
    "for index, row in df0.iterrows():\n",
    "\n",
    "    #文本\n",
    "    text = row['text']\n",
    "\n",
    "    #分割位置\n",
    "    indices = find_sep(text)\n",
    "\n",
    "    # 根据分割位置生成data\n",
    "    generate_data(indices)\n",
    "    \n",
    "# 保存为DataFrame\n",
    "df = pd.DataFrame(data, columns = ['length', 'text', 'labels'])\n",
    "\n",
    "# 按句子长度倒序排列\n",
    "df = df.sort_values(by='length', ascending=False)\n",
    "\n",
    "# 保存\n",
    "df.to_csv('data/sep_max_'+str(max_size)+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载分词器并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/rbt6',pad_to_max_length=False)\n",
    "\n",
    "# 测试文本\n",
    "text = [\"红土创新基金管理有限公司6月30日发布公告，红土创新盐田港REIT(180301)的基金经理新聘陈超。\"\n",
    ",\"金能科技: 金能科技股份有限公司关于“金能转债”转股价格调整的提示性公告\"]\n",
    "\n",
    "# 将文本进行逐字分解\n",
    "def split_sent(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "\n",
    "def split(text_list):\n",
    "\n",
    "    text_split = []\n",
    "    for sent in text_list:\n",
    "        sent_split = split_sent(sent)\n",
    "        text_split.append(sent_split)\n",
    "    return text_split\n",
    "\n",
    "\n",
    "# 分解并编码\n",
    "def split_and_encode(text_list):\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "    split(text_list),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# 展示编码结果\n",
    "inputs_test = split_and_encode(text)\n",
    "for key,value in inputs_test.items():\n",
    "    print(key ,\": \\n\", value, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、定义新的Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Dataset类，用于处理训练集\"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        # 用pandas读取csv，并将其转换为Dataset\n",
    "        data_df = pd.read_csv(path)\n",
    "        data_list = data_df.to_dict(orient=\"list\")\n",
    "        dataset = datasets.Dataset.from_dict(data_list)\n",
    "\n",
    "        #过滤掉太长的句子\n",
    "        def f(data):\n",
    "            return data['length'] <= max_size - 2\n",
    "\n",
    "        dataset = dataset.filter(f)\n",
    "\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        token = split_sent(self.data[index]['text'])\n",
    "        label = eval(self.data[index]['labels'])\n",
    "\n",
    "        return token, label \n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = Dataset('data/sep_max_'+str(max_size)+'.csv')\n",
    "\n",
    "# 使用数据集\n",
    "token, label = dataset[0]\n",
    "\n",
    "# 展示\n",
    "len(dataset), list(token), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、定义数据整理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pad = constants[\"pad\"]\n",
    "\n",
    "#数据整理函数\n",
    "def collate_fn(data):\n",
    "    \n",
    "    # tokens是分解后的文本\n",
    "    tokens = [row[0] for row in data]\n",
    "    # labels是对应的标记\n",
    "    labels = [row[1] for row in data]\n",
    "\n",
    "    # inputs是tokens的编码\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True)\n",
    "\n",
    "    # 编码的长度（最长长度）\n",
    "    length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # 将[CLS]和[PAD]标记，并将标记长度与编码长度统一\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [pad] + labels[i]\n",
    "        labels[i] += [pad] * length\n",
    "        labels[i] = labels[i][:length]\n",
    "\n",
    "    return inputs, torch.LongTensor(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、定义数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批大小\n",
    "batch_size = constants[\"batch_size\"]\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "#查看数据样例\n",
    "for i, (inputs, labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(len(labels))\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "for k, v in inputs.items():\n",
    "    print(k,'\\t', v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加载预训练模型\n",
    "pretrained = AutoModel.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000, '万')\n",
    "\n",
    "#模型试算\n",
    "pretrained(**inputs).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、定义下游主体模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "out_len = constants[\"out_len\"]\n",
    "\n",
    "# 定义运行的设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device1 = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    \"\"\"下游命名实体识别模型\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # tuning用于调整模式，False表示使用外部预训练模型，True表示使用本模型\n",
    "        self.tuneing = False\n",
    "\n",
    "        # 模型初始化\n",
    "        self.pretrained = None\n",
    "\n",
    "        # 添加RNN层\n",
    "        self.rnn = torch.nn.GRU(768, 768, batch_first=True)\n",
    "\n",
    "        # 添加全连接层\n",
    "        self.fc = torch.nn.Linear(768, out_len)\n",
    "\n",
    "\n",
    "    # 传播函数\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc(out).softmax(dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    # 调整tuning的函数\n",
    "    def fine_tuneing(self, tuneing):\n",
    "\n",
    "        self.tuneing = tuneing\n",
    "\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "\n",
    "pretrained = pretrained.to(device)\n",
    "\n",
    "# 创建模型实例\n",
    "model = Model()\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、定义辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1）对计算结果和label变形,并且移除pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "\n",
    "    #变形,便于计算loss\n",
    "    #[b, lens, out_len] -> [b*lens, out_len]\n",
    "    outs = outs.reshape(-1, out_len)\n",
    "    \n",
    "    #[b, lens] -> [b*lens]\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    #[b, lens] -> [b*lens - pad]\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）获取识别正确数量和总数，包括总体和除零两种情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_and_total_count(labels, outs):\n",
    "\n",
    "    #[b*lens, out_len] -> [b*lens]\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    # 计算除了0以外元素的正确率,因为0太多了,包括的话,正确率很容易虚高\n",
    "    select = (labels == B_ENT) + (labels == I_ENT)\n",
    "\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    print('正确数:', correct, '总数:', total, '去零正确数:', correct_content, '去零总数:', total_content)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "# 测试\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, out_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入优化器\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train(epochs):\n",
    "\n",
    "    \"\"\"训练函数\"\"\"\n",
    "\n",
    "    # 学习率\n",
    "    lr = 2e-5 if model.tuneing else 5e-4\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # 损失函数\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for step, (inputs, labels) in enumerate(loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            #模型计算\n",
    "            #[b, lens] -> [b, lens, out_len]\n",
    "            outs = model(inputs)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #对outs和label变形,并且移除pad\n",
    "            #outs -> [b, lens, out_len] -> [c, out_len]\n",
    "            #labels -> [b, lens] -> [c]\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels, inputs['attention_mask'])\n",
    "\n",
    "            #梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 每50步输出一次训练效果\n",
    "            if step % 50 == 0:\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "                accuracy = counts[0] / counts[1]\n",
    "                accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "                print('轮数:', epoch, '批数:', step, '损失:', loss.item(), '准确率:', accuracy, '去零准确率:', accuracy_content)\n",
    "\n",
    "        # 模型保存\n",
    "        torch.save(model, 'model/model1.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、非tuning训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fine_tuneing(False)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、tuning训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    \"\"\"测试函数\"\"\"\n",
    "\n",
    "    # 导入训练好的模型\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    # 数据加载器\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                              batch_size=128,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    # 变量初始化\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "\n",
    "        # 测试5批\n",
    "        if step == 5:\n",
    "            break\n",
    "\n",
    "        print('批数:', step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            model_load = model_load.to(device)\n",
    "\n",
    "            #[b, lens] -> [b, lens, out_len] -> [b, lens]\n",
    "            outs = model_load(inputs)\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "\n",
    "        #对outs和label变形,并且移除pad\n",
    "        #outs -> [b, lens, out_len] -> [c, out_len]\n",
    "        #labels -> [b, lens] -> [c]\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels, inputs['attention_mask'])\n",
    "\n",
    "        # 获取识别正确数量和总数\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print('总准确率:', correct / total,'去零总准确率:', correct_content / total_content)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、手动预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_one():\n",
    "\n",
    "    # 导入训练好的模型\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.eval()\n",
    "\n",
    "\n",
    "    # 自定义文本\n",
    "    text = [\"红土创新基金管理有限公司6月30日发布公告，红土创新盐田港REIT(180301)的基金经理新聘陈超。\"\n",
    "    ,\"金能科技: 金能科技股份有限公司关于“金能转债”转股价格调整的提示性公告\",\n",
    "    \"大地熊成功收购技研株式会社安徽大地熊新材料股份有限公司2023年6月22日,大地熊日本株式会社与技研株式会社\"\\\n",
    "        \"(英文名:p.m.giken inc,简称pm公司)原有股东和相关方签订收购合同》,大地熊日本株式会社全资收购pm公司\"\\\n",
    "        \"并即时启动相关手续交接。 pm公司是专业从事磁钢整体磁气回路的解析与设计,注塑磁体的开发、制造与销售。\"\\\n",
    "        \"此次收购是大地熊向注塑磁体领域的全新产业布局,将进一步推进大地熊海内外战略合作与业务拓展,丰富公司磁性材料及部件产品种类,提升公司综合实力和整体竞争力。\",\n",
    "    \"黑猫股份:设立全资子公司投建年产16万吨碳材/橡胶复合母胶项目证券时报e公司讯,黑猫股份(002068)6月27日晚间\"\\\n",
    "    \"公告,公司拟出资1亿元在辽宁省朝阳市高新技术产业开发区设立全资子公司辽宁黑猫复合新材料科技有限公司(简称辽\"\\\n",
    "    \"宁黑猫),并且将以辽宁黑猫为项目主体,投资新建年产16万吨碳材/橡胶复合母胶项目,分三期进行建设,项目预计投资总额为6.88亿元。\",\n",
    "    \"秋田满满、小鹿蓝蓝食安问题频发,投诉也没用随着我国新生代父母的科学喂养观念加强,我国婴童辅食行业消费规模\"\\\n",
    "        \"持续上升。早在2019年我国婴幼儿辅食消费市场规模就已经达到404亿元,年复合增长率高达23%,预计未来我国婴幼儿辅食市场规模应在千亿以上。\",\n",
    "        \"非银金融600218全柴动力46875.42 13.57 -1.30汽车002166莱茵生物46891.42 13.40 -3.47\"]\n",
    "    \n",
    "    text_split = []\n",
    "    for sent in text:\n",
    "        sent_split = [char for char in sent]\n",
    "        text_split.append(sent_split)\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "    text_split,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "        inputs = inputs.to(device)\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "\n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''), len(input_id))\n",
    "\n",
    "        #输出tag\n",
    "        s = ''\n",
    "        for j in range(len(out)):\n",
    "            if out[j] == 0:\n",
    "                s += '·'\n",
    "                continue\n",
    "            s += tokenizer.decode(input_id[j])\n",
    "            s += str(out[j].item())\n",
    "\n",
    "        print(s)\n",
    "        print('==========================')\n",
    "    \n",
    "predict_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、测试集预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/rbt6')\n",
    "\n",
    "# 分句\n",
    "def separate(text):\n",
    "\n",
    "    sent_list = []\n",
    "\n",
    "    indices = find_sep(text)\n",
    "\n",
    "    for j in range(len(indices)-1):\n",
    "\n",
    "        begin = indices[j] + 1\n",
    "        end = indices[j+1] + 1\n",
    "        sent = text[begin: end]\n",
    "\n",
    "        sent_list.append(sent)\n",
    "\n",
    "    return sent_list\n",
    "\n",
    "\n",
    "# 将完整的label和text转换为mentions\n",
    "def label_to_mention(text, labels):\n",
    "    \n",
    "    mention_list = []\n",
    "\n",
    "    length = len(labels)\n",
    "\n",
    "    for index in range(length):\n",
    "\n",
    "        if labels[index] == B_ENT:\n",
    "            begin = index\n",
    "            entity_index = index\n",
    "\n",
    "            while labels[entity_index] != 0 and entity_index < length:\n",
    "                entity_index += 1\n",
    "\n",
    "            end = entity_index\n",
    "    \n",
    "            entity = text[begin: end]\n",
    "\n",
    "            mention = {\"mention\": entity, \"offset\": (begin, end)}\n",
    "\n",
    "            mention_list.append(mention)\n",
    "\n",
    "    return mention_list\n",
    "\n",
    "\n",
    "# 预测函数\n",
    "def predict():\n",
    "\n",
    "    # 导入模型\n",
    "    model_load = torch.load('model/model.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    # 导入测试集\n",
    "    data = pd.read_excel('data/test.xlsx')\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        print(index)\n",
    "\n",
    "        text = row['text']\n",
    "\n",
    "        # 分句\n",
    "        sent_list = separate(text)\n",
    "        labels = []\n",
    "\n",
    "        # 分词\n",
    "        text_split = []\n",
    "        for sent in sent_list:\n",
    "            sent = replace_space(sent)\n",
    "            sent_split = [char for char in sent]\n",
    "            text_split.append(sent_split)\n",
    "\n",
    "        # 编码\n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text_split,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors='pt',\n",
    "            is_split_into_words=True)\n",
    "\n",
    "        # 计算\n",
    "        with torch.no_grad():\n",
    "            #[b, lens] -> [b, lens, 8] -> [b, lens]\n",
    "            inputs = inputs.to(device)\n",
    "            outs = model_load(inputs).argmax(dim=2)\n",
    "            inputs = inputs.to(device1)\n",
    "            outs = outs.to(device1)\n",
    "\n",
    "        # 标记处理\n",
    "        for i in range(len(text_split)):\n",
    "            #移除pad, cls和sep\n",
    "            select = inputs['attention_mask'][i] == 1\n",
    "            out = outs[i, select]\n",
    "            label = out.tolist()[1: len(out)-1]\n",
    "            labels += label\n",
    "\n",
    "        # 保存labels和mentions\n",
    "        data.loc[index, \"labels\"] = str(labels)\n",
    "        mentions = label_to_mention(text, labels)\n",
    "        data.loc[index, \"mentions\"] = str(mentions)\n",
    "\n",
    "    # 保存为文件\n",
    "    data.to_csv('data/out.csv')\n",
    "\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、知识库查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# 加载测试集预测结果\n",
    "df = pd.read_csv('data/out.csv')\n",
    "\n",
    "# 加载知识库\n",
    "kb = pd.read_excel('data/knowledge_base_new.xlsx')\n",
    "\n",
    "# 在知识库中查找并将知识添加到knowledge_list中\n",
    "def find(entity, offset):\n",
    "    for index, row in kb.iterrows():\n",
    "        if index not in index_list and (row['stockName'] == entity or row['companyName'] == entity):\n",
    "            know = \\\n",
    "            {\"companyId\": row['companyId'],\n",
    "            \"companyName\": row['companyName'],\n",
    "            \"mention\": entity, \n",
    "            \"offset\": offset, \n",
    "            \"stockId\": int(row['stockId']) if not math.isnan(row['stockId']) else 'null', \n",
    "            \"stockName\": row['stockName'] if not math.isnan(row['stockId']) else 'null', \n",
    "            \"type\": row['type'][:2]}\n",
    "            knowledge_list.append(know)\n",
    "            index_list.append(index)\n",
    "        \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    print(index)\n",
    "\n",
    "    mention_list = eval(row['mentions'])\n",
    "\n",
    "    # 存储找过的entity，避免重复\n",
    "    entity_list = []\n",
    "\n",
    "    # 存储已找到的knowledge下标，避免重复\n",
    "    index_list = []\n",
    "\n",
    "    # 用于存储找到的knowledge\n",
    "    knowledge_list = []\n",
    "\n",
    "    # 对mention列表的每一个mention做处理，其中每个mention是一个dict\n",
    "    for mention in mention_list:\n",
    "\n",
    "        entity = mention['mention']\n",
    "\n",
    "        offset = mention['offset']\n",
    "\n",
    "        if entity not in entity_list:\n",
    "\n",
    "            find(entity, offset)\n",
    "\n",
    "            entity_list.append(entity)\n",
    "\n",
    "    # 存储结果\n",
    "    df.loc[index, \"result\"] = str(knowledge_list)\n",
    "\n",
    "# 保存为文件\n",
    "df.to_csv('data/knowledge_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
